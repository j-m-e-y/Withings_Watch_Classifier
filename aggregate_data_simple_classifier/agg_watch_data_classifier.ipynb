{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Watch Data Classification Project\n",
    "\n",
    "The goal of this project is to investigate and utilize the data collected from a personal smartwatch to provide daily workout recommendations. Using the data collected from the Withings brand watch, we want to predict whether or not a person will have a successful workout on a given day. Providing this insight to users in the morning could provide valuable information about how the user could structure their day or provide the necessary motivation to make a workout routine become a workout habit. The idea of a \"successful workout\" will be investigated as well as which data provides insights in workout performance during the next day. \n",
    "\n",
    "As an initial analysis, the data that has been aggregated by day will be used to determine whether or not it is a good predictor of a workout the following day, additionally the sleep data will be organized and cleaned to provide additional insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data\n",
    "\n",
    "In this case, we are only looking at the data that has already been aggregated by day and not every file included in the watch_data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = pd.read_csv(\"../watch_data/aggregates_distance.csv\", header=0)\n",
    "\n",
    "passive_calories = pd.read_csv(\"../watch_data/aggregates_calories_passive.csv\", header=0)\n",
    "active_calories = pd.read_csv(\"../watch_data/aggregates_calories_earned.csv\", header=0)\n",
    "\n",
    "steps = pd.read_csv(\"../watch_data/aggregates_steps.csv\", header=0)\n",
    "\n",
    "sleep_data = pd.read_csv(\"../watch_data/sleep.csv\", header=0)\n",
    "\n",
    "workouts = pd.read_csv(\"../watch_data/activities.csv\", header=0)\n",
    "\n",
    "# Aggregate if I want to do same operation on all DataFrames\n",
    "datasets = [distance, passive_calories, active_calories, steps, sleep_data, workouts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation/Cleaning\n",
    "\n",
    "Now that we have imported all of the relevant data, we need to clean the data and prepare it for model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df:pd.DataFrame):\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)\n",
    "    elif 'from' in df.columns:\n",
    "        df['from'] = pd.to_datetime(df['from'], infer_datetime_format=True, utc=True)\n",
    "        df['to'] = pd.to_datetime(df['to'], infer_datetime_format=True, utc=True)\n",
    "    else:\n",
    "        print('No columns defined as date/from/to in {}'.format(df))\n",
    "\n",
    "\n",
    "# Start with the simple files, check for nans and turn the date columns into datetime types\n",
    "for df in datasets:\n",
    "    convert_to_datetime(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.rename(columns={'value':'distance'}, inplace=True)\n",
    "passive_calories.rename(columns={'value':'passive calories'}, inplace=True)\n",
    "active_calories.rename(columns={'value':'active calories'}, inplace=True)\n",
    "steps.rename(columns={'value':'steps'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can focus on the \"workouts\" dataset. This dataset will require more work to get into a useable format. We will first drop the columns that contain only NaN values that provide no additional information that can be inferred or "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts.drop(['from (manual)', 'to (manual)','GPS', 'Modified'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's clear that a lot of information is located in the \"Data\" column of the workouts DataFrame and needs to be unpacked. Many of the elements in the Data column are the empty set, we also need to investigate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of times the Data column is the empty array\n",
    "(workouts['Data'] == '{}').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe the \"Other\" workout types result in this output, however there are only 114 \"Other\" workouts registered\n",
    "workouts['Activity type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing which type of workouts results in the null bracket for the workout\n",
    "workouts[workouts['Data'] == '{}']['Activity type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The empty bracket categories are somewhat random, let's unpack the Data column to see what it contains for the different workouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "ast.literal_eval(workouts['Data'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see that the 'effduration' category is simply the number of seconds the workout is\n",
    "(workouts['to'].iloc[0] - workouts['from'].iloc[0]).total_seconds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a first investigation before diving into the 'Data' column too heavily as it changes for different workout types, we can simply compute the workout duration and save it as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column that generates a TimeDelta datetime object\n",
    "workouts['Duration'] = (workouts['to'] - workouts['from']) / np.timedelta64(1, 's')\n",
    "workouts['Duration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here it becomes more clear that the workouts with the empty brackets are simply duplicates of the previous workouts without the data, so we can remove each of these rows from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workouts = workouts[workouts['Data'] != '{}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's make a few changes to simplify the workouts data.\n",
    "1. Replace the \"from\" and \"to\" columns with one date that indicates the date of the workout in addition to the \"Duration\" column\n",
    "2. Remove the \"Data\" column (further investigation at a later point)\n",
    "\n",
    "Care is needed because there may be multiple workouts in one day. In this case, if multiple workouts occur on the same day, they will be summed into one day and one duration value. In this case, we will also drop the activity type and timezone and simply find the total duration for each day that a workout was completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicworkouts = workouts.copy()\n",
    "basicworkouts['date'] = pd.to_datetime(workouts['from'].dt.date)\n",
    "basicworkouts.drop(['from', 'to', 'Data', 'Timezone', 'Activity type'], axis=1, inplace=True)\n",
    "basicworkouts.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum workout duration on the days when there are more than one workout and then consider a workout effective if it lasts longer than the 30 minutes recommended by CDC\n",
    "basicworkouts = basicworkouts.groupby(['date']).sum()\n",
    "basicworkouts['Effective Workout'] = basicworkouts['Duration'] > 1800\n",
    "basicworkouts['Effective Workout'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, we need to clean that sleep_data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's clear from the sleep data that REM sleep is not recorded, as well as snoring, snoring episodes, and night events. These columns can be dropped. Additionally, \"from\" and \"to\" columns are not necessary as we simply want the date that the sleep occurred that corresponds with a workout later that day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data.drop(['rem (s)', 'Snoring (s)', 'Snoring episodes', 'Night events'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data['date'] = pd.to_datetime(sleep_data[\"from\"].dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we look at the values for the dates, we see that we have one day in which there are 2 recorded sleeps! Let's investigate this date\n",
    "sleep_data['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data[sleep_data['date'] == '20220127']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### After investigating, it's clear that the duplicate was due to a long nap I took on vacation :) $\\quad$  I will remove this sleep record from the data set in this case. In other applications, it may become necessary to create some outlier detection to determine when a sleep event occurs outside of the usual time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep_data.drop(499, inplace=True)\n",
    "sleep_data.drop(['from', 'to'], axis=1, inplace=True)\n",
    "sleep_data.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaned sleep data\n",
    "sleep_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that the more complicated datasets have been cleaned, let's quickly clean the distance, passive_calories, active_calories, and steps sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.set_index('date', inplace=True)\n",
    "passive_calories.set_index('date', inplace=True)\n",
    "active_calories.set_index('date', inplace=True)\n",
    "steps.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicworkouts.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's join the datasets together on the date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = pd.merge(distance, passive_calories, how='inner', on='date')\n",
    "X_data = pd.merge(X_data, active_calories, how='inner', on='date')\n",
    "X_data = pd.merge(X_data, steps, how='inner', on='date')\n",
    "X_data = pd.merge(X_data, sleep_data, how='inner', on='date')\n",
    "X_data = pd.merge(X_data, basicworkouts, how='inner', on='date')\n",
    "X_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis\n",
    "\n",
    "### Now that the data has been cleaned to a usable format, we can briefly explore the data before applying different ML techniques. It should be noted that this data will only include the days in which data exists for each of the previous datasets, i.e. days in which I had a workout, recorded my sleep, and other data exists (recorded automatically every day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 6))\n",
    "axes.scatter(X_data.index, X_data['steps'])\n",
    "axes.set_xlabel('Date')\n",
    "axes.set_ylabel('Steps')\n",
    "\n",
    "axes.set_title(\"My Steps on Workout Days Over Time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "steps_by_day = X_data.groupby(X_data.index.day_name()).mean().reindex(cats) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 6))\n",
    "axes.bar(steps_by_day.index, steps_by_day['steps'])\n",
    "axes.set_xlabel('Day of the Week')\n",
    "axes.set_ylabel('Steps')\n",
    "\n",
    "axes.set_title(\"Average Steps on Workout Days by Day of the Week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.hist(bins=50, figsize=(20,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Exploration/Engineering\n",
    "## Many of the techniques have been chosen from the text \"Hands on Machine Learning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = X_data.corr()\n",
    "corr_mat['Effective Workout'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After looking at the correlations, it's clear that some of the data would not make sense to predict whether or not someone will have an effective workout. After consideration, it makes sense to predict whether an effective workout will occur or not based off of information from the sleep data OR data from the previous day. Because of this, the following steps will be made to modify the data:\n",
    "\n",
    "1. The Duration, distance, active calories, steps, and passive calories categories will be removed from the current day as they occur concurrently with the current day's workout and may be confounding variables\n",
    "2. The distance, active/passive calories, and steps from the previous day will be added in as possible influence over an effective workout or not\n",
    "3. The total time asleep is added as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove confounding variables\n",
    "x_test = X_data.drop(['Duration', 'distance', 'active calories', 'steps', 'passive calories'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add total time asleep as a column\n",
    "x_test['total sleep'] = X_data['light (s)'] + X_data['deep (s)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "prev_day_index = x_test.index - timedelta(days=1)\n",
    "x_test['prevday steps'] = steps['steps'].loc[prev_day_index].values\n",
    "x_test['prevday active cals'] = active_calories['active calories'].loc[prev_day_index].values\n",
    "x_test['prevday passive cals'] = passive_calories['passive calories'].loc[prev_day_index].values\n",
    "x_test['prevday distance'] = distance['distance'].loc[prev_day_index].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = x_test.corr()\n",
    "corr_mat['Effective Workout'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can plot the correlation matrix with all of our data we will use to predict an effective workout.\n",
    "\n",
    "From this data, the main correlation to \"Effective Workout\" come from the sleep data with the highest correlation related to light sleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepping Data for Fitting Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(x_test, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_set['Effective Workout'].values.astype(int)\n",
    "x_train = train_set.drop(['Effective Workout'], axis=1)\n",
    "\n",
    "y_test = test_set['Effective Workout'].values.astype(int)\n",
    "x_test = test_set.drop(['Effective Workout'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "x_train_prepared = num_pipeline.fit_transform(x_train)\n",
    "x_test_prepared  = num_pipeline.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the classifier in the standard way\n",
    "from sklearn.linear_model import SGDClassifier \n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(x_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the cross-validation score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, x_train_prepared, y_train, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the confusion matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_train_pred = cross_val_predict(sgd_clf, x_train_prepared, y_train, cv=3)\n",
    "\n",
    "\n",
    "y_scores = cross_val_predict(sgd_clf, x_train_prepared, y_train, cv=3, method=\"decision_function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the results we see:\n",
    "\n",
    "1. Our classifier performs equally poorly at Type 1 and 2 errors and that our resulting precision and recall is very similar. This may mean that our SGD classifier can do no better than what is shown here without tradeoffs between precision and recall\n",
    "2. The average cross-validation accuracy of around 0.63 is somewhat better than random chance, but doesn't tell the full story because the precision and recall are decent compared with the accuracy\n",
    "3. Precision: 0.763\n",
    "4. Recall   : 0.756\n",
    "5. F1 score : 0.7596\n",
    "6. The classifier is not good at predicting true negative, or cases when a poor workout is expected. This could mean that we need to train a different model, or that the input features are simply not good predictors of a good workout or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What would the accuracy be if we simply guessed a good workout every time?\n",
    "sum(np.ones(len(y_train)) == y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\n",
    "\n",
    "def plot_precision_vs_recall(precisions, recalls):\n",
    "    plt.plot(recalls, precisions, \"b-\", linewidth=2)\n",
    "    plt.xlabel(\"Recall\", fontsize=16)\n",
    "    plt.ylabel(\"Precision\", fontsize=16)\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plot_precision_vs_recall(precisions, recalls)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can look at the ROC curve to see the performance of our SGD classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])                                    \n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=16) \n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=16)    \n",
    "    plt.grid(True)                                            \n",
    "\n",
    "plt.figure(figsize=(8, 6))                                    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From this initial investigation, the current classifier performs poorly. This could be due to a number of factors:\n",
    "1. Small datasets (more data may differentiate bad workouts from good workouts more)\n",
    "2. Class imbalance (75% are effective workouts and 25% are not)\n",
    "3. Incorrect metrics (perhaps a 30 minute workout is not the sure-fire metric that was expected)\n",
    "\n",
    "## First, other binary classifiers will be tested and then further analysis will be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('ml_projects': conda)",
   "name": "python395jvsc74a57bd0edfa0b2b2ff525f667f4204c7d9a2f4c4c6fe95b72ffcaf5eac44e529307b759"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "edfa0b2b2ff525f667f4204c7d9a2f4c4c6fe95b72ffcaf5eac44e529307b759"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}